import requests
from bs4 import BeautifulSoup
import time
import re
import csv
from typing import List, Dict

# –ö–ª–∞—Å—Å-–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è Rusprofile, –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–∞—Ä—Å–∏–Ω–≥
class RusprofileParser:
    def __init__(self):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω RusprofileParser (–∑–∞–≥–ª—É—à–∫–∞)")
    
    def get_financials(self, inn):
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ rusprofile.ru
        # –°–µ–π—á–∞—Å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        return {
            'inn': inn,
            'name': f'–ö–æ–º–ø–∞–Ω–∏—è_{inn}',
            'revenue': 150000000,  # –í—Å–µ–≥–¥–∞ > 100 –º–ª–Ω –¥–ª—è —Ç–µ—Å—Ç–∞
            'employees': 50,
            'site': f'www.company{inn}.ru',
            'cat_product': 'Trados/Smartcat',
            'okved_main': '62.01'
        }

class SuperJobParser:
    BASE_URL = "https://www.superjob.ru/vakansii/"
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def search_companies_by_keywords(self, keywords: List[str], max_results: int = 30) -> List[Dict]:
        companies = {}
        session = requests.Session()
        
        for keyword in keywords:
            print(f"üîç –ü–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤—É: '{keyword}'...")
            search_query = keyword.replace(' ', '+')
            url = f"{self.BASE_URL}?keywords={search_query}&town=4"
            
            try:
                response = session.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤!)
                vacancy_items = soup.find_all('div', class_='f-test-search-result-item')
                if not vacancy_items:
                    vacancy_items = soup.find_all('div', {'class': re.compile(r'.*search-result-item.*')})
                
                for item in vacancy_items[:max_results]:
                    company_block = item.find('span', class_='f-test-text-vacancy-item-company-name')
                    if not company_block:
                        continue
                    
                    company_name = company_block.get_text(strip=True)
                    if not company_name or company_name.lower() == '—Å–∫—Ä—ã—Ç–æ':
                        continue
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ò–ù–ù –¥–ª—è —Ç–µ—Å—Ç–∞ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∏—Å–∫–∞—Ç—å –Ω–∞ rusprofile)
                    company_id = re.sub(r'\W+', '_', company_name.lower())
                    fake_inn = ''.join([str(ord(c) % 10) for c in company_name[:10]]).ljust(10, '0')[:10]
                    
                    if company_id not in companies:
                        companies[company_id] = {
                            'name': company_name,
                            'inn': fake_inn,  # –¢–µ—Å—Ç–æ–≤—ã–π –ò–ù–ù
                            'source': 'superjob.ru',
                            'cat_evidence': f'–í–∞–∫–∞–Ω—Å–∏—è: {keyword}',
                            'keywords_found': [keyword]
                        }
                    else:
                        if keyword not in companies[company_id]['keywords_found']:
                            companies[company_id]['keywords_found'].append(keyword)
                            companies[company_id]['cat_evidence'] += f', {keyword}'
                
                time.sleep(1)
                
            except Exception as e:
                print(f"   –û—à–∏–±–∫–∞: {e}")
                continue
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
        result_list = list(companies.values())
        for company in result_list:
            company.pop('keywords_found', None)
        
        return result_list

# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
    # pip install requests beautifulsoup4 lxml
    
    cat_keywords = [
        "Trados", "memoQ", "Smartcat", "Crowdin", "Phrase",
        "translation memory", "–ø–∞–º—è—Ç—å –ø–µ—Ä–µ–≤–æ–¥–æ–≤", "CAT tool",
        "–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è", "–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å"
    ]
    
    parser = SuperJobParser()
    print("–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ SuperJob.ru...")
    found_companies = parser.search_companies_by_keywords(cat_keywords, max_results=15)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(found_companies)}")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Rusprofile (–∑–∞–≥–ª—É—à–∫–∞)
    rusprofile = RusprofileParser()
    filtered_companies = []
    
    for company in found_companies:
        inn = company.get('inn')
        if inn:
            financials = rusprofile.get_financials(inn)
            if financials and financials.get('revenue', 0) >= 100_000_000:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                company.update(financials)
                filtered_companies.append(company)
    
    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_companies)} –∫–æ–º–ø–∞–Ω–∏–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if filtered_companies:
        with open('superjob_companies.csv', 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['inn', 'name', 'revenue', 'site', 'source', 
                         'cat_evidence', 'cat_product', 'employees', 'okved_main']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(filtered_companies)
        
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ superjob_companies.csv")
        for i, company in enumerate(filtered_companies[:3]):
            print(f"   {i+1}. {company['name']} - {company['revenue']} —Ä—É–±.")
    else:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    pass


